{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:\\\\Users\\\\Michelle\\\\Documents\\\\Cummins\\\\Sem6\\\\JOCPython\\\\2020-4-14_docs\\\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flipping an Image \n",
    "We use the Pillow or PIL module in python to flip the image <br>\n",
    "Directions that we can transpose the image are:\n",
    "- PIL.Image.FLIP_LEFT_RIGHT : mirror image from left to right \n",
    "- PIL.Image.FLIP_TOP_BOTTOM : mirror image upside down\n",
    "- PIL.Image.ROTATE_90 : rotate 90 deg anticlockwise\n",
    "- PIL.Image.ROTATE_180 : rotate 180 deg anticlockwise - same as FLIP_LEFT_RIGHT then FLIP_TOP_BOTTOM\n",
    "- PIL.Image.ROTATE_270 : rotate 270 anticlockwise\n",
    "- PIL.Image.TRANSPOSE : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "my_img = Image.open(path+\"original2.png\")\n",
    "transposed_img = my_img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "transposed_img = transposed_img.transpose(Image.ROTATE_270)\n",
    "#there is no FLIP_RIGHT_LEFT - which would technically also have worked\n",
    "transposed_img.save(path+\"flipped.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Histogram \n",
    "What we will cover: \n",
    "- CLAHE Contrast Limited Adaptive Histogram Equalization\n",
    "- How to flip the image - two ways\n",
    "\n",
    "**Image Histogram**<br>\n",
    "Type of histogram that acts as a graphical representation of the tonal distribution in a digital image. plots the number of pixels for each tonal value. You can judge the entire tonal distribution at a glance. <br>\n",
    "Horizontal axis - tonal variations, Vertial axis - number of pixels. The left side of the horizontal axis represents the dark areas, middle represents the mid-tone values, and the right hand side represents the light areas.<br>\n",
    "If a dark image, most of the data points will be on the left side. A bright image will have few dark areas and shadows will have most of its data points on the right side and center of the graph. <br>\n",
    "\n",
    "**Histogram equalization**<br>\n",
    "Trying to operate on the image so that the tonal values / contrast of the image is equally distributed in the histogram. <br>\n",
    "Problem: if there is a small area then histogram equalization may work but if you have a large area then (high contrast) you might have a problem. <br>\n",
    "Thats how *adaptive equalization* came about. The picture is divided into tiles and histogram equalization is applied to each tile. One method of adaptive histogram equalization is CLAHE.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colour image-blue1.png: Its shape is  (531, 1890, 3) \n",
      "\n",
      "Image matrix of colour image is [[[27 28 10]\n",
      "  [27 28 10]\n",
      "  [26 26 10]\n",
      "  ...\n",
      "  [35 34 11]\n",
      "  [35 34 11]\n",
      "  [35 34 11]]\n",
      "\n",
      " [[27 28 10]\n",
      "  [27 28 10]\n",
      "  [27 28 10]\n",
      "  ...\n",
      "  [35 34 11]\n",
      "  [35 34 11]\n",
      "  [32 30 11]]\n",
      "\n",
      " [[27 28 10]\n",
      "  [26 26 10]\n",
      "  [26 26 10]\n",
      "  ...\n",
      "  [35 34 11]\n",
      "  [35 34 11]\n",
      "  [32 33  9]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[31 35 15]\n",
      "  [31 35 15]\n",
      "  [33 39 15]\n",
      "  ...\n",
      "  [35 37 10]\n",
      "  [33 39 15]\n",
      "  [33 39 15]]\n",
      "\n",
      " [[31 35 15]\n",
      "  [33 39 15]\n",
      "  [33 39 15]\n",
      "  ...\n",
      "  [35 37 11]\n",
      "  [33 36 11]\n",
      "  [33 36 11]]\n",
      "\n",
      " [[31 35 15]\n",
      "  [33 39 15]\n",
      "  [31 34 15]\n",
      "  ...\n",
      "  [33 36 11]\n",
      "  [33 36 11]\n",
      "  [33 36 11]]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "#read the image - read as an image object\n",
    "img = cv2.imread(path+\"blur1.png\")\n",
    "#print the shape of the matrix row col channel (colours-3=RGB,0=Gray)\n",
    "print(\"Colour image-blue1.png: Its shape is \",img.shape,\"\\n\")\n",
    "#print the numpy matrix of the image\n",
    "print(\"Image matrix of colour image is\",img,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cv2.cvtColor(src, code[, dst[, dstCn]])<br>\n",
    "- src: input image: 8-bit unsigned, 16-bit unsigned ( CV_16UC... ), or single-precision floating-point.\n",
    "- dst: output image of the same size and depth as src\n",
    "\n",
    "- code: color space conversion code (see the description below)\n",
    "    - CV_BGR2GRAY, CV_RGB2GRAY, CV_GRAY2BGR, CV_GRAY2RGB \n",
    "    - difference between CV_BGR2GRAY and CV_RGB2GRAY: It's to do with how the colors are stored. The default pixel format of OpenCV is BGR so the first byte in a standard (24-bit) color image will be an 8-bit Blue component, the second byte will be Green, and the third byte will be Red. The opposite will be for RGB \n",
    "    - use GBR whenever needed. Only if you have obtained your image in a different format, used RGB\n",
    "    \n",
    "- dstCn: number of channels in the destination image; if the parameter is 0, the number of the channels is derived automatically from src and code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converted gray image of blur1.png's shape:  (531, 1890) \n",
      "\n",
      "Image matrix of converted gray image-blur1.png is [[23 23 21 ... 27 27 27]\n",
      " [23 23 23 ... 27 27 25]\n",
      " [23 21 21 ... 27 27 26]\n",
      " ...\n",
      " [29 29 31 ... 29 31 31]\n",
      " [29 31 31 ... 29 28 28]\n",
      " [29 31 28 ... 28 28 28]] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creat a clahe method \n",
    "clahe = cv2.createCLAHE()\n",
    "#for this method we need to convert the coloured image to grayscale\n",
    "#we use cvtColor()\n",
    "gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "#saved the converted gray image\n",
    "cv2.imwrite(path+\"blur1_gray.png\",gray_img)\n",
    "\n",
    "#a gray image does not print the channel/color code in shape: only row and coloumn is displayed\n",
    "print(\"converted gray image of blur1.png's shape: \",gray_img.shape,\"\\n\")\n",
    "print(\"Image matrix of converted gray image-blur1.png is\",gray_img,\"\\n\")\n",
    "\n",
    "#enhance the image on the gray image\n",
    "enh_img = clahe.apply(gray_img)\n",
    "cv2.imwrite(path+\"blur1_gray_en.png\",enh_img)\n",
    "\n",
    "#enh_color_img = clahe.apply(img) - gives an error when you try to apply clahe to a colored image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are there any methods with which we can enhance a coloured image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
