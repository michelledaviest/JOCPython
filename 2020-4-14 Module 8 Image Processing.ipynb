{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:\\\\Users\\\\Michelle\\\\Documents\\\\Cummins\\\\Sem6\\\\JOCPython\\\\2020-4-14_docs\\\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flipping an Image \n",
    "We use the Pillow or PIL module in python to flip the image <br>\n",
    "Directions that we can transpose the image are:\n",
    "- PIL.Image.FLIP_LEFT_RIGHT : mirror image from left to right \n",
    "- PIL.Image.FLIP_TOP_BOTTOM : mirror image upside down\n",
    "- PIL.Image.ROTATE_90 : rotate 90 deg anticlockwise\n",
    "- PIL.Image.ROTATE_180 : rotate 180 deg anticlockwise - same as FLIP_LEFT_RIGHT then FLIP_TOP_BOTTOM\n",
    "- PIL.Image.ROTATE_270 : rotate 270 anticlockwise \n",
    "- PIL.Image.TRANSPOSE : same as FLIP_LEFT_RIGHT then ROTATE_90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "my_img = Image.open(path+\"original2.png\")\n",
    "transposed_img = my_img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "#there is no FLIP_RIGHT_LEFT - which would technically also have worked\n",
    "transposed_img.save(path+\"flipped2.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Histogram \n",
    "What we will cover: \n",
    "- CLAHE Contrast Limited Adaptive Histogram Equalization\n",
    "- How to flip the image - two ways\n",
    "\n",
    "**Image Histogram**<br>\n",
    "Type of histogram that acts as a graphical representation of the tonal distribution in a digital image. plots the number of pixels for each tonal value. You can judge the entire tonal distribution at a glance. <br>\n",
    "Horizontal axis - tonal variations, Vertial axis - number of pixels. The left side of the horizontal axis represents the dark areas, middle represents the mid-tone values, and the right hand side represents the light areas.<br>\n",
    "If a dark image, most of the data points will be on the left side. A bright image will have few dark areas and shadows will have most of its data points on the right side and center of the graph. <br>\n",
    "\n",
    "**Histogram equalization**<br>\n",
    "Trying to operate on the image so that the tonal values / contrast of the image is equally distributed in the histogram. <br>\n",
    "Problem: if there is a small area then histogram equalization may work but if you have a large area then (high contrast) you might have a problem. <br>\n",
    "Thats how *adaptive equalization* came about. The picture is divided into tiles and histogram equalization is applied to each tile. One method of adaptive histogram equalization is CLAHE.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colour image-blue1.png: Its shape is  (531, 1890, 3) \n",
      "\n",
      "Image matrix of colour image is [[[27 28 10]\n",
      "  [27 28 10]\n",
      "  [26 26 10]\n",
      "  ...\n",
      "  [35 34 11]\n",
      "  [35 34 11]\n",
      "  [35 34 11]]\n",
      "\n",
      " [[27 28 10]\n",
      "  [27 28 10]\n",
      "  [27 28 10]\n",
      "  ...\n",
      "  [35 34 11]\n",
      "  [35 34 11]\n",
      "  [32 30 11]]\n",
      "\n",
      " [[27 28 10]\n",
      "  [26 26 10]\n",
      "  [26 26 10]\n",
      "  ...\n",
      "  [35 34 11]\n",
      "  [35 34 11]\n",
      "  [32 33  9]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[31 35 15]\n",
      "  [31 35 15]\n",
      "  [33 39 15]\n",
      "  ...\n",
      "  [35 37 10]\n",
      "  [33 39 15]\n",
      "  [33 39 15]]\n",
      "\n",
      " [[31 35 15]\n",
      "  [33 39 15]\n",
      "  [33 39 15]\n",
      "  ...\n",
      "  [35 37 11]\n",
      "  [33 36 11]\n",
      "  [33 36 11]]\n",
      "\n",
      " [[31 35 15]\n",
      "  [33 39 15]\n",
      "  [31 34 15]\n",
      "  ...\n",
      "  [33 36 11]\n",
      "  [33 36 11]\n",
      "  [33 36 11]]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "#read the image - read as an image object\n",
    "img = cv2.imread(path+\"blur1.png\")\n",
    "#print the shape of the matrix row col channel (colours-3=RGB,0=Gray)\n",
    "print(\"Colour image-blue1.png: Its shape is \",img.shape,\"\\n\")\n",
    "#print the numpy matrix of the image\n",
    "print(\"Image matrix of colour image is\",img,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cv2.cvtColor(src, code[, dst[, dstCn]])<br>\n",
    "- src: input image: 8-bit unsigned, 16-bit unsigned ( CV_16UC... ), or single-precision floating-point.\n",
    "- dst: output image of the same size and depth as src\n",
    "\n",
    "- code: color space conversion code (see the description below)\n",
    "    - CV_BGR2GRAY, CV_RGB2GRAY, CV_GRAY2BGR, CV_GRAY2RGB \n",
    "    - difference between CV_BGR2GRAY and CV_RGB2GRAY: It's to do with how the colors are stored. The default pixel format of OpenCV is BGR so the first byte in a standard (24-bit) color image will be an 8-bit Blue component, the second byte will be Green, and the third byte will be Red. The opposite will be for RGB \n",
    "    - use GBR whenever needed. Only if you have obtained your image in a different format, used RGB\n",
    "    \n",
    "- dstCn: number of channels in the destination image; if the parameter is 0, the number of the channels is derived automatically from src and code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converted gray image of blur1.png's shape:  (531, 1890) \n",
      "\n",
      "Image matrix of converted gray image-blur1.png is [[23 23 21 ... 27 27 27]\n",
      " [23 23 23 ... 27 27 25]\n",
      " [23 21 21 ... 27 27 26]\n",
      " ...\n",
      " [29 29 31 ... 29 31 31]\n",
      " [29 31 31 ... 29 28 28]\n",
      " [29 31 28 ... 28 28 28]] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creat a clahe method \n",
    "clahe = cv2.createCLAHE()\n",
    "#for this method we need to convert the coloured image to grayscale\n",
    "#we use cvtColor()\n",
    "gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "#saved the converted gray image\n",
    "cv2.imwrite(path+\"blur1_gray.png\",gray_img)\n",
    "\n",
    "#a gray image does not print the channel/color code in shape: only row and coloumn is displayed\n",
    "print(\"converted gray image of blur1.png's shape: \",gray_img.shape,\"\\n\")\n",
    "print(\"Image matrix of converted gray image-blur1.png is\",gray_img,\"\\n\")\n",
    "\n",
    "#enhance the image on the gray image\n",
    "enh_img = clahe.apply(gray_img)\n",
    "cv2.imwrite(path+\"blur1_gray_en.png\",enh_img)\n",
    "\n",
    "#enh_color_img = clahe.apply(img) - gives an error when you try to apply clahe to a colored image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are there any methods with which we can enhance a coloured image?<br>\n",
    "PIL ImageEnhance module (https://pillow.readthedocs.io/en/3.0.x/reference/ImageEnhance.html)<br>\n",
    "There are 4 classes : Contrast, Color, Brightness, Sharpness classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Brightness class** <br>\n",
    "ImageEnhance.Brightness.enhance(factor)<br>\n",
    "factor of 0.0 gives a black image, factor of 1.0 gives back the original image, as you increase the factor the image tends to white. At which factor the image would turn white would depend on the original brightness in the image. For blur1.png, it is somewhere between 25 and 30. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageEnhance\n",
    "img = Image.open(path+\"blur1.png\")\n",
    "#create object of Brightness class\n",
    "im_b = ImageEnhance.Brightness(img)\n",
    "#enhance the image by a factor of 2.0\n",
    "img_enhance = im_b.enhance(2.0)\n",
    "img_enhance.save(path+\"blur1_enh_pil.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Contrast class** <br>\n",
    "ImageEnhance.Contrast.enhance(factor)<br>\n",
    "This class can be used to control the contrast of an image, similar to the contrast control on a TV set. Increase the brightness in the bright parts and decrease it in the dark parts. An enhancement factor of 0.0 gives a solid grey image. A factor of 1.0 gives the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_c = ImageEnhance.Contrast(img).enhance(2.0)\n",
    "im_c.save(path+\"blur1_contrast_pil.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Color class** (Saturation)<br>\n",
    "ImageEnhance.Color.enhance(factor)<br>\n",
    "Adjust image color balance.\n",
    "This class can be used to adjust the colour balance of an image, in a manner similar to the controls on a colour TV set. An enhancement factor of 0.0 gives a black and white image. A factor of 1.0 gives the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_col = ImageEnhance.Color(img).enhance(3.0)\n",
    "im_col.save(path+\"blur1_col_pil.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sharpness class** <br>\n",
    "ImageEnhance.Sharpness.enhance(factor)<br>\n",
    "Adjust image sharpness.\n",
    "This class can be used to adjust the sharpness of an image. An enhancement factor of 0.0 gives a blurred image, a factor of 1.0 gives the original image. Increasing the factor increases the sharpness of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_s = ImageEnhance.Sharpness(img).enhance(10.0)\n",
    "im_s.save(path+\"blur1_sharp_pil.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "im.show(img) will open the file in the default image viewer and on closing the window will save the image to the file system. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
